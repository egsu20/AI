{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "11. pneumonia",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1VQ6G-xcEVJ2bSRPD3bINv3tJOf_NjwMo",
      "authorship_tag": "ABX9TyPfdoBwRUv3gCSpJvhjXla9"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH2zyY-hxFj4"
      },
      "source": [
        "### Keras CNN으로 폐렴 X-Ray 구분하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt20Ngn6xMU_"
      },
      "source": [
        "1. 패키지 수입 및 하이퍼 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euhJuOVHvUah"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import os\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from time import time\r\n",
        "from sklearn.metrics import confusion_matrix, f1_score\r\n",
        "\r\n",
        "from keras.models import Model\r\n",
        "from keras.layers import Conv2D, SeparableConv2D\r\n",
        "from keras.layers import Input, Flatten\r\n",
        "from keras.layers import BatchNormalization, MaxPool2D # 매 층마다 z-정규화\r\n",
        "from keras.layers import Dense, Dropout\r\n",
        "from keras.preprocessing.image import load_img\r\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPCn9KAOypaY"
      },
      "source": [
        "# 하이퍼 파라미터 설정\r\n",
        "MY_EPOCH = 200\r\n",
        "MY_BATCH = 100\r\n",
        "MY_RES = 180\r\n",
        "MY_SHAPE = (MY_RES, MY_RES, 3) # 칼라 이미지\r\n",
        "\r\n",
        "# 모드 설정\r\n",
        "DATA_MODE = 1\r\n",
        "TRAIN_MODE = 1\r\n",
        "MY_PATH = '/content/drive/MyDrive/Colab Notebooks/data/chest'\r\n",
        "\r\n",
        "# keras.io에서 pneumonia Classfication 참고하기"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHK1SXSUFsoK"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvKJYe-YFsYK"
      },
      "source": [
        "# 변수 정의\r\n",
        "# T_tot : train total, 학습용 이미지 개수\r\n",
        "# V_tot : validation total, 평가용 이미지 개수\r\n",
        "# N_tot : normal total, 정상 이미지 수\r\n",
        "# P_tot : pnuemonia total, 폐렴 이미지 수\r\n",
        "# N_path : normal image path, 정상 이미지 경로\r\n",
        "# P_path : pneumonia image path, 폐렴 이미지 경로\r\n",
        "\r\n",
        "# 전역 변수 설정\r\n",
        "T_tot = V_tot = N_tot = P_tot = 0\r\n",
        "N_path = P_path = []\r\n",
        "\r\n",
        "X_train = np.zeros((0)) # 아무것도 들어있지 않은 0차원 np\r\n",
        "Y_train = np.zeros((0)) \r\n",
        "X_test = np.zeros((0)) \r\n",
        "Y_test = np.zeros((0))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq8NDPpXGxJ5"
      },
      "source": [
        "# 학습용 입력 이미지 경로 처리\r\n",
        "def train_path():\r\n",
        "    global T_tot, N_tot, P_tot, N,path, P_path\r\n",
        "\r\n",
        "    # 정상 이미지 처리\r\n",
        "    dir = os.path.join(MY_PATH, 'train/NORMAL') # 경로 두개를 합침\r\n",
        "    print(dir)\r\n",
        "    \r\n",
        "    # 이미지 파일 경로\r\n",
        "    N_path = []\r\n",
        "    for f in os.listdir(dir):\r\n",
        "        N_path.append(os.path.join(dir, f)) # dir과 파일 이름을 합침 -> 파일의 총 경로가 됨\r\n",
        "    N_tot = len(N_path)\r\n",
        "    print('학습용 정상 이미지 수 :', N_tot)\r\n",
        "    \r\n",
        "    # 폐렴 이미지 처리\r\n",
        "    dir = os.path.join(MY_PATH, 'train/PNEUMONIA')\r\n",
        "    print(dir)\r\n",
        "\r\n",
        "    # 이미지 파일 경로\r\n",
        "    P_path = []\r\n",
        "    for f in os.listdir(dir):\r\n",
        "        P_path.append(os.path.join(dir, f)) # dir과 파일 이름을 합침 -> 파일의 총 경로가 됨\r\n",
        "    P_tot = len(P_path)\r\n",
        "    print('학습용 폐렴 이미지 수 :', P_tot)\r\n",
        "\r\n",
        "    # 총 학습용 이미지 수\r\n",
        "    T_tot = N_tot + P_tot\r\n",
        "    print('학습용 이미지 수 :', T_tot)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLo97bv8MNM3"
      },
      "source": [
        "# 학습용 이미지 데이터 처리 (화상도 통일)\r\n",
        "# 정상 라벨은 0, 폐렴은 1\r\n",
        "def train_resize():\r\n",
        "    global T_tot, N_tot, P_tot, N,path, P_path\r\n",
        "    global X_train, Y_train\r\n",
        "\r\n",
        "    print('정상 이미지 처리 시작')\r\n",
        "    begin = time()\r\n",
        "\r\n",
        "    # 초기화\r\n",
        "    X_train = np.zeros((T_tot, MY_RES, MY_RES, 3)) # 데이터 수, 이미지 화소수, 칼라\r\n",
        "    Y_train = np.zeros((T_tot,)) # 1차원 벡터\r\n",
        "\r\n",
        "    for i, path in enumerate(N_path):    \r\n",
        "        #tmp = load_img(path)\r\n",
        "        #print(np.array(tmp).shape)\r\n",
        "        #plt.imshow(tmp)\r\n",
        "        #plt.show()\r\n",
        "\r\n",
        "        # 이미지 크기 조정\r\n",
        "        img = load_img(path,\r\n",
        "                       target_size=(MY_RES, MY_RES))\r\n",
        "        \r\n",
        "        #plt.imshow(tmp)\r\n",
        "        #plt.show()\r\n",
        "\r\n",
        "        X_train[i] = img\r\n",
        "        Y_train[i] = 0 # 라벨 정보\r\n",
        "        #print(X_train[i].shape)\r\n",
        "\r\n",
        "    end = time()\r\n",
        "    print('정상 이미지 처리 소요 시간 : {:.2f}'.format(end - begin))\r\n",
        "\r\n",
        "    print('폐렴 이미지 처리 시작')\r\n",
        "    begin = time()\r\n",
        "\r\n",
        "    for i, path in enumerate(P_path):\r\n",
        "        img = load_img(path, \r\n",
        "                       target_size=(MY_RES, MY_RES))\r\n",
        "        X_train[i + N_tot] = img \r\n",
        "        Y_train[i + N_tot] = 1 # 폐렴은 1\r\n",
        "\r\n",
        "    end = time()\r\n",
        "    print('폐렴 이미지 처리 소요 시간 : {:.2f}'.format(end - begin))\r\n",
        "\r\n",
        "    # [0, 255] 사이의 8-비트 칼라 정보를 [0,1]로 스케일링(정규화)\r\n",
        "    #print(X_train[0])\r\n",
        "    X_train = X_train / 255.0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TXVmzaNifGC"
      },
      "source": [
        "# 평가용 데이터 경로 처리\r\n",
        "def test_path():\r\n",
        "    global V_tot, N_tot, P_tot, N,path, P_path\r\n",
        "\r\n",
        "    # 정상 이미지 처리\r\n",
        "    dir = os.path.join(MY_PATH, 'test/NORMAL') # 경로 두개를 합침\r\n",
        "    print(dir)\r\n",
        "    \r\n",
        "    # 이미지 파일 경로\r\n",
        "    N_path = []\r\n",
        "    for f in os.listdir(dir):\r\n",
        "        N_path.append(os.path.join(dir, f)) # dir과 파일 이름을 합침 -> 파일의 총 경로가 됨\r\n",
        "    N_tot = len(N_path)\r\n",
        "    print('평가용 정상 이미지 수 :', N_tot)\r\n",
        "    \r\n",
        "    # 폐렴 이미지 처리\r\n",
        "    dir = os.path.join(MY_PATH, 'test/PNEUMONIA')\r\n",
        "    print(dir)\r\n",
        "\r\n",
        "    # 이미지 파일 경로\r\n",
        "    P_path = []\r\n",
        "    for f in os.listdir(dir):\r\n",
        "        P_path.append(os.path.join(dir, f)) # dir과 파일 이름을 합침 -> 파일의 총 경로가 됨\r\n",
        "    P_tot = len(P_path)\r\n",
        "    print('평가용 폐렴 이미지 수 :', P_tot)\r\n",
        "\r\n",
        "    # 총 평가용 이미지 수\r\n",
        "    V_tot = N_tot + P_tot\r\n",
        "    print('평가용 이미지 수 :', V_tot)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e064ZR3plBgp"
      },
      "source": [
        "# 평가용 이미지 데이터 처리 (화상도 통일)\r\n",
        "# 정상 라벨은 0, 폐렴은 1\r\n",
        "def test_resize():\r\n",
        "    global V_tot, N_tot, P_tot, N,path, P_path\r\n",
        "    global X_test, Y_test\r\n",
        "\r\n",
        "    print('정상 이미지 처리 시작')\r\n",
        "    begin = time()\r\n",
        "\r\n",
        "    # 초기화\r\n",
        "    X_test = np.zeros((V_tot, MY_RES, MY_RES, 3)) # 데이터 수, 이미지 화소수, 칼라\r\n",
        "    Y_test = np.zeros((V_tot,)) # 1차원 벡터\r\n",
        "\r\n",
        "    for i, path in enumerate(N_path):    \r\n",
        "        #tmp = load_img(path)\r\n",
        "        #print(np.array(tmp).shape)\r\n",
        "        #plt.imshow(tmp)\r\n",
        "        #plt.show()\r\n",
        "\r\n",
        "        # 이미지 크기 조정(축소)\r\n",
        "        img = load_img(path,\r\n",
        "                       target_size=(MY_RES, MY_RES))\r\n",
        "        \r\n",
        "        #plt.imshow(tmp)\r\n",
        "        #plt.show()\r\n",
        "\r\n",
        "        X_test[i] = img\r\n",
        "        Y_test[i] = 0 # 라벨 정보\r\n",
        "        #print(X_train[i].shape)\r\n",
        "\r\n",
        "    end = time()\r\n",
        "    print('정상 이미지 처리 소요 시간 : {:.2f}'.format(end - begin))\r\n",
        "\r\n",
        "    print('폐렴 이미지 처리 시작')\r\n",
        "    begin = time()\r\n",
        "\r\n",
        "    for i, path in enumerate(P_path):\r\n",
        "        img = load_img(path, \r\n",
        "                       target_size=(MY_RES, MY_RES))\r\n",
        "        X_test[i + N_tot] = img \r\n",
        "        Y_test[i + N_tot] = 1 # 폐렴은 1\r\n",
        "\r\n",
        "    end = time()\r\n",
        "    print('폐렴 이미지 처리 소요 시간 : {:.2f}'.format(end - begin))\r\n",
        "\r\n",
        "    # [0, 255] 사이의 8-비트 칼라 정보를 [0,1]로 스케일링(정규화)\r\n",
        "    #print(X_test[0])\r\n",
        "    X_test = X_test / 255.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASNaoJ9fnNzZ"
      },
      "source": [
        "# 사분할 데이터 저장\r\n",
        "def save_data():\r\n",
        "    with open('chest-arrays.npy', 'wb') as f:\r\n",
        "      np.save(f, X_train)\r\n",
        "      np.save(f, Y_train)\r\n",
        "      np.save(f, X_test)\r\n",
        "      np.save(f, Y_test)\r\n",
        "\r\n",
        "    print('사분할 데이터 파일 완성')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjtxbfzspsGW"
      },
      "source": [
        "# 사분할 데이터 읽기\r\n",
        "def read_data():\r\n",
        "    global X_train, Y_train, X_test, Y_test\r\n",
        "\r\n",
        "    with open('chest-arrays.npy', 'rb') as f:\r\n",
        "        X_train = np.load(f)\r\n",
        "        Y_train = np.load(f)\r\n",
        "        X_test = np.load(f)\r\n",
        "        Y_test = np.load(f)\r\n",
        "\r\n",
        "    print('사분할 데이터 파일 읽기 완성')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsg_deeSqP5-",
        "outputId": "6e2ab4e1-f6cb-40eb-8797-cf3b8195c4b1"
      },
      "source": [
        "# 데이터 처리 컨트롤 타워\r\n",
        "if DATA_MODE:\r\n",
        "    train_path()\r\n",
        "    train_resize()\r\n",
        "    test_path()\r\n",
        "    test_resize()\r\n",
        "    save_data()\r\n",
        "else:\r\n",
        "    read_data()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/data/chest/train/NORMAL\n",
            "학습용 정상 이미지 수 : 500\n",
            "/content/drive/MyDrive/Colab Notebooks/data/chest/train/PNEUMONIA\n",
            "학습용 폐렴 이미지 수 : 500\n",
            "학습용 이미지 수 : 1000\n",
            "정상 이미지 처리 시작\n",
            "정상 이미지 처리 소요 시간 : 0.38\n",
            "폐렴 이미지 처리 시작\n",
            "폐렴 이미지 처리 소요 시간 : 440.54\n",
            "/content/drive/MyDrive/Colab Notebooks/data/chest/test/NORMAL\n",
            "평가용 정상 이미지 수 : 100\n",
            "/content/drive/MyDrive/Colab Notebooks/data/chest/test/PNEUMONIA\n",
            "평가용 폐렴 이미지 수 : 100\n",
            "평가용 이미지 수 : 200\n",
            "정상 이미지 처리 시작\n",
            "정상 이미지 처리 소요 시간 : 0.04\n",
            "폐렴 이미지 처리 시작\n",
            "폐렴 이미지 처리 소요 시간 : 83.03\n",
            "사분할 데이터 파일 완성\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3GXTRHIvq0k",
        "outputId": "8546deba-cba1-4ba3-c45a-56faf2be43e3"
      },
      "source": [
        "# 데이터 모양 확인\r\n",
        "print('학습용 입력 데이터 모양:', X_train.shape)\r\n",
        "print('학습용 출력 데이터 모양:', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터 모양:', X_test.shape)\r\n",
        "print('평가용 출력 데이터 모양:', Y_test.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 모양: (1000, 180, 180, 3)\n",
            "학습용 출력 데이터 모양: (1000,)\n",
            "평가용 입력 데이터 모양: (200, 180, 180, 3)\n",
            "평가용 출력 데이터 모양: (200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7WCHzoB01Lh"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0OAqHHG042p"
      },
      "source": [
        "# 합성곱 블럭 함수\r\n",
        "def conv_block(filters, inputs):\r\n",
        "    # Conv2D 빠른 버전 \r\n",
        "    x = SeparableConv2D(filters=filters,\r\n",
        "                        kernel_size=3,\r\n",
        "                        activation='relu',\r\n",
        "                        padding='same')(inputs)\r\n",
        "\r\n",
        "    x = SeparableConv2D(filters=filters,\r\n",
        "                        kernel_size=3,\r\n",
        "                        activation='relu',\r\n",
        "                        padding='same')(x) # 전 단의 아웃풋이 현재 단의 인풋으로\r\n",
        "\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "\r\n",
        "    output = MaxPool2D(pool_size=2)(x)\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1VNINq94pxJ"
      },
      "source": [
        "# Dense 블럭 함수\r\n",
        "def dense_block(units, drop, inputs):\r\n",
        "    x = Dense(units=units,\r\n",
        "              activation='relu')(inputs)\r\n",
        "\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "\r\n",
        "    output = Dropout(drop)(x)\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7M2yqNFC5N9f",
        "outputId": "7ca89c8a-7acc-4ebc-c214-560ac46ba999"
      },
      "source": [
        "# 전체 CNN 구현\r\n",
        "\r\n",
        "# 입력층\r\n",
        "my_input = Input(shape=(MY_SHAPE)) # 180*180*3\r\n",
        "\r\n",
        "# 블럭 2\r\n",
        "x = Conv2D(filters=16,\r\n",
        "           kernel_size=3,\r\n",
        "           activation='relu',\r\n",
        "           padding='same')(my_input)\r\n",
        "\r\n",
        "x = Conv2D(filters=16,\r\n",
        "           kernel_size=3,\r\n",
        "           activation='relu',\r\n",
        "           padding='same')(x)\r\n",
        "\r\n",
        "x = MaxPool2D(pool_size=2)(x) # 2배로 줄어듦\r\n",
        "\r\n",
        "# 블럭 3\r\n",
        "x = conv_block(32, x) # 전 단의 출력이 다음 단의 입력\r\n",
        "\r\n",
        "# 블럭 4\r\n",
        "x = conv_block(64, x)\r\n",
        "\r\n",
        "# 블럭 5\r\n",
        "x = conv_block(128, x)\r\n",
        "\r\n",
        "# 블럭 6\r\n",
        "x = conv_block(256, x)\r\n",
        "x = Dropout(0.2)(x)\r\n",
        "\r\n",
        "# 블럭 7\r\n",
        "x = Flatten()(x)\r\n",
        "\r\n",
        "# 블럭 8\r\n",
        "x = dense_block(512, 0.7, x) \r\n",
        "x = dense_block(128, 0.5, x)\r\n",
        "x = dense_block(64, 0.3, x)\r\n",
        "\r\n",
        "# 블럭 9\r\n",
        "my_output = Dense(units=1,\r\n",
        "          activation='sigmoid')(x)\r\n",
        "\r\n",
        "# 모델 만들기\r\n",
        "model = Model(inputs=my_input,\r\n",
        "              outputs=my_output)\r\n",
        "\r\n",
        "print('CNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN 요약\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 180, 180, 3)]     0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 180, 180, 16)      448       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 180, 180, 16)      2320      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 90, 90, 16)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d (SeparableC (None, 90, 90, 32)        688       \n",
            "_________________________________________________________________\n",
            "separable_conv2d_1 (Separabl (None, 90, 90, 32)        1344      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 90, 90, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 45, 45, 32)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_2 (Separabl (None, 45, 45, 64)        2400      \n",
            "_________________________________________________________________\n",
            "separable_conv2d_3 (Separabl (None, 45, 45, 64)        4736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 45, 45, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_4 (Separabl (None, 22, 22, 128)       8896      \n",
            "_________________________________________________________________\n",
            "separable_conv2d_5 (Separabl (None, 22, 22, 128)       17664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 22, 22, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 11, 11, 128)       0         \n",
            "_________________________________________________________________\n",
            "separable_conv2d_6 (Separabl (None, 11, 11, 256)       34176     \n",
            "_________________________________________________________________\n",
            "separable_conv2d_7 (Separabl (None, 11, 11, 256)       68096     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 11, 11, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 5, 5, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               3277312   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 3,496,801\n",
            "Trainable params: 3,494,433\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUIZTASb_afW"
      },
      "source": [
        "4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKQ8VtnT_duP",
        "outputId": "11defe16-655e-4cce-87d2-2f5a7c8e8f95"
      },
      "source": [
        "# CNN 학습\r\n",
        "model.compile(optimizer='adam',\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['acc'])\r\n",
        "\r\n",
        "# 학습 진행\r\n",
        "begin = time()\r\n",
        "print('학습 시작')\r\n",
        "\r\n",
        "model.fit(x=X_train,\r\n",
        "          y=Y_train,\r\n",
        "          epochs=MY_EPOCH,\r\n",
        "          batch_size=MY_BATCH,\r\n",
        "          verbose=1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습 시간 : {:.2f}초'.format(end-begin))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습 시작\n",
            "Epoch 1/200\n",
            "10/10 [==============================] - 14s 492ms/step - loss: 0.3924 - acc: 0.8014\n",
            "Epoch 2/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 0.0154 - acc: 0.9993\n",
            "Epoch 3/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 0.0118 - acc: 1.0000\n",
            "Epoch 4/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 5/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 6/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 7/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 8/200\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 9/200\n",
            "10/10 [==============================] - 5s 501ms/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 10/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 11/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 0.0018 - acc: 1.0000\n",
            "Epoch 12/200\n",
            "10/10 [==============================] - 5s 508ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 13/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 14/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 15/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 16/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 0.0017 - acc: 1.0000\n",
            "Epoch 17/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 8.2930e-04 - acc: 1.0000\n",
            "Epoch 18/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 8.9403e-04 - acc: 1.0000\n",
            "Epoch 19/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 20/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 6.4613e-04 - acc: 1.0000\n",
            "Epoch 21/200\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 8.0637e-04 - acc: 1.0000\n",
            "Epoch 22/200\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 9.1133e-04 - acc: 1.0000\n",
            "Epoch 23/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 6.8151e-04 - acc: 1.0000\n",
            "Epoch 24/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 8.5812e-04 - acc: 1.0000\n",
            "Epoch 25/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 7.8174e-04 - acc: 1.0000\n",
            "Epoch 26/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 6.2101e-04 - acc: 1.0000\n",
            "Epoch 27/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 0.0013 - acc: 1.0000\n",
            "Epoch 28/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 3.6923e-04 - acc: 1.0000\n",
            "Epoch 29/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 5.3140e-04 - acc: 1.0000\n",
            "Epoch 30/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 4.7704e-04 - acc: 1.0000\n",
            "Epoch 31/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 5.9122e-04 - acc: 1.0000\n",
            "Epoch 32/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 4.4204e-04 - acc: 1.0000\n",
            "Epoch 33/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 5.7752e-04 - acc: 1.0000\n",
            "Epoch 34/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 3.9592e-04 - acc: 1.0000\n",
            "Epoch 35/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 5.8042e-04 - acc: 1.0000\n",
            "Epoch 36/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 8.0886e-04 - acc: 1.0000\n",
            "Epoch 37/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 3.8716e-04 - acc: 1.0000\n",
            "Epoch 38/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 3.9192e-04 - acc: 1.0000\n",
            "Epoch 39/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 3.6198e-04 - acc: 1.0000\n",
            "Epoch 40/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 2.6268e-04 - acc: 1.0000\n",
            "Epoch 41/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 3.6751e-04 - acc: 1.0000\n",
            "Epoch 42/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 6.4354e-04 - acc: 1.0000\n",
            "Epoch 43/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 3.5219e-04 - acc: 1.0000\n",
            "Epoch 44/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 2.7215e-04 - acc: 1.0000\n",
            "Epoch 45/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 2.5741e-04 - acc: 1.0000\n",
            "Epoch 46/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 3.0723e-04 - acc: 1.0000\n",
            "Epoch 47/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 4.2347e-04 - acc: 1.0000\n",
            "Epoch 48/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 3.1128e-04 - acc: 1.0000\n",
            "Epoch 49/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 2.4700e-04 - acc: 1.0000\n",
            "Epoch 50/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 1.5384e-04 - acc: 1.0000\n",
            "Epoch 51/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 2.5597e-04 - acc: 1.0000\n",
            "Epoch 52/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.3311e-04 - acc: 1.0000\n",
            "Epoch 53/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.3639e-04 - acc: 1.0000\n",
            "Epoch 54/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 2.6133e-04 - acc: 1.0000\n",
            "Epoch 55/200\n",
            "10/10 [==============================] - 5s 502ms/step - loss: 2.5517e-04 - acc: 1.0000\n",
            "Epoch 56/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 1.6951e-04 - acc: 1.0000\n",
            "Epoch 57/200\n",
            "10/10 [==============================] - 5s 504ms/step - loss: 2.3936e-04 - acc: 1.0000\n",
            "Epoch 58/200\n",
            "10/10 [==============================] - 5s 501ms/step - loss: 1.9773e-04 - acc: 1.0000\n",
            "Epoch 59/200\n",
            "10/10 [==============================] - 5s 505ms/step - loss: 1.5655e-04 - acc: 1.0000\n",
            "Epoch 60/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 1.6444e-04 - acc: 1.0000\n",
            "Epoch 61/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 2.4664e-04 - acc: 1.0000\n",
            "Epoch 62/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 1.7354e-04 - acc: 1.0000\n",
            "Epoch 63/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 2.0495e-04 - acc: 1.0000\n",
            "Epoch 64/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 1.4914e-04 - acc: 1.0000\n",
            "Epoch 65/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.7507e-04 - acc: 1.0000\n",
            "Epoch 66/200\n",
            "10/10 [==============================] - 5s 484ms/step - loss: 1.7341e-04 - acc: 1.0000\n",
            "Epoch 67/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 2.2076e-04 - acc: 1.0000\n",
            "Epoch 68/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 2.1155e-04 - acc: 1.0000\n",
            "Epoch 69/200\n",
            "10/10 [==============================] - 5s 506ms/step - loss: 2.3584e-04 - acc: 1.0000\n",
            "Epoch 70/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 1.0379e-04 - acc: 1.0000\n",
            "Epoch 71/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 1.4010e-04 - acc: 1.0000\n",
            "Epoch 72/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 2.0143e-04 - acc: 1.0000\n",
            "Epoch 73/200\n",
            "10/10 [==============================] - 5s 478ms/step - loss: 2.3353e-04 - acc: 1.0000\n",
            "Epoch 74/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 4.3751e-04 - acc: 1.0000\n",
            "Epoch 75/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 1.3851e-04 - acc: 1.0000\n",
            "Epoch 76/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 1.1439e-04 - acc: 1.0000\n",
            "Epoch 77/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 1.6190e-04 - acc: 1.0000\n",
            "Epoch 78/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 1.6452e-04 - acc: 1.0000\n",
            "Epoch 79/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.4952e-04 - acc: 1.0000\n",
            "Epoch 80/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 1.0143e-04 - acc: 1.0000\n",
            "Epoch 81/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 1.4930e-04 - acc: 1.0000\n",
            "Epoch 82/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 1.8550e-04 - acc: 1.0000\n",
            "Epoch 83/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 1.4539e-04 - acc: 1.0000\n",
            "Epoch 84/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 1.1105e-04 - acc: 1.0000\n",
            "Epoch 85/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 1.5057e-04 - acc: 1.0000\n",
            "Epoch 86/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 2.0242e-04 - acc: 1.0000\n",
            "Epoch 87/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 1.0341e-04 - acc: 1.0000\n",
            "Epoch 88/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.3424e-04 - acc: 1.0000\n",
            "Epoch 89/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 1.6893e-04 - acc: 1.0000\n",
            "Epoch 90/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 1.0111e-04 - acc: 1.0000\n",
            "Epoch 91/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 2.4077e-04 - acc: 1.0000\n",
            "Epoch 92/200\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 1.1537e-04 - acc: 1.0000\n",
            "Epoch 93/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 9.2412e-05 - acc: 1.0000\n",
            "Epoch 94/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 1.0597e-04 - acc: 1.0000\n",
            "Epoch 95/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 1.1342e-04 - acc: 1.0000\n",
            "Epoch 96/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.1878e-04 - acc: 1.0000\n",
            "Epoch 97/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 1.0005e-04 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 9.6867e-05 - acc: 1.0000\n",
            "Epoch 99/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 1.0174e-04 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 1.0531e-04 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 7.5547e-05 - acc: 1.0000\n",
            "Epoch 102/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 1.4236e-04 - acc: 1.0000\n",
            "Epoch 103/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 1.5377e-04 - acc: 1.0000\n",
            "Epoch 104/200\n",
            "10/10 [==============================] - 5s 473ms/step - loss: 1.1071e-04 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 5.6623e-05 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 1.0077e-04 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "10/10 [==============================] - 5s 483ms/step - loss: 1.1171e-04 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 1.3797e-04 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 5.1088e-05 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.6675e-04 - acc: 1.0000\n",
            "Epoch 111/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 1.1257e-04 - acc: 1.0000\n",
            "Epoch 112/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 1.9895e-04 - acc: 1.0000\n",
            "Epoch 113/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 6.0232e-05 - acc: 1.0000\n",
            "Epoch 114/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 8.9945e-05 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 3.3104e-04 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 8.5081e-05 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 8.3646e-05 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "10/10 [==============================] - 5s 505ms/step - loss: 7.3558e-05 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 6.0085e-05 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 7.7783e-05 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 5.0314e-05 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 1.0051e-04 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 9.1896e-05 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 7.2348e-05 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.0146e-04 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "10/10 [==============================] - 5s 504ms/step - loss: 1.3704e-04 - acc: 1.0000\n",
            "Epoch 127/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 4.0302e-05 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 9.3384e-05 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 7.3653e-05 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 8.6546e-05 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "10/10 [==============================] - 5s 486ms/step - loss: 5.9803e-05 - acc: 1.0000\n",
            "Epoch 132/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 4.0716e-05 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 8.1221e-05 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 5.0635e-05 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 7.0011e-05 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 7.8692e-05 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 3.9326e-05 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.7528e-05 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 6.0035e-05 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 1.4145e-04 - acc: 1.0000\n",
            "Epoch 141/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 4.9554e-05 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "10/10 [==============================] - 5s 482ms/step - loss: 2.2338e-04 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 5.5516e-05 - acc: 1.0000\n",
            "Epoch 144/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 6.5694e-05 - acc: 1.0000\n",
            "Epoch 145/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 5.4362e-05 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "10/10 [==============================] - 5s 501ms/step - loss: 3.9957e-05 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 3.8742e-05 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 4.1972e-05 - acc: 1.0000\n",
            "Epoch 149/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 6.0095e-05 - acc: 1.0000\n",
            "Epoch 150/200\n",
            "10/10 [==============================] - 5s 480ms/step - loss: 7.4902e-05 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 7.6916e-05 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 3.6385e-05 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "10/10 [==============================] - 5s 507ms/step - loss: 5.3955e-05 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "10/10 [==============================] - 5s 500ms/step - loss: 7.4103e-05 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 3.5454e-05 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 3.0028e-05 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 3.3833e-05 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 6.7481e-05 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 5.3805e-05 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 5.0539e-05 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 3.1956e-05 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "10/10 [==============================] - 5s 474ms/step - loss: 4.3624e-05 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 4.1742e-05 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 8.1090e-05 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 5.0696e-05 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 6.2949e-05 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 6.1240e-05 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 7.5883e-05 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 3.4483e-05 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 3.0227e-05 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "10/10 [==============================] - 5s 476ms/step - loss: 3.0198e-05 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 6.1938e-05 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 3.5659e-05 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 5.7039e-05 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "10/10 [==============================] - 5s 497ms/step - loss: 3.6628e-05 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "10/10 [==============================] - 5s 487ms/step - loss: 2.9483e-05 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 8.5446e-05 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 1.9745e-05 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 3.6382e-05 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 2.0421e-05 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 2.9851e-05 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 2.1977e-05 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "10/10 [==============================] - 5s 488ms/step - loss: 4.1577e-05 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 3.7586e-05 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 2.3129e-05 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 4.8278e-05 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 4.6597e-05 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "10/10 [==============================] - 5s 504ms/step - loss: 5.1498e-05 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 3.1080e-05 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "10/10 [==============================] - 5s 498ms/step - loss: 3.0610e-05 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "10/10 [==============================] - 5s 499ms/step - loss: 2.3784e-05 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "10/10 [==============================] - 5s 495ms/step - loss: 4.1479e-05 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "10/10 [==============================] - 5s 491ms/step - loss: 2.4780e-05 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "10/10 [==============================] - 5s 485ms/step - loss: 7.7001e-05 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "10/10 [==============================] - 5s 493ms/step - loss: 6.9681e-05 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "10/10 [==============================] - 5s 490ms/step - loss: 1.9766e-05 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "10/10 [==============================] - 5s 492ms/step - loss: 3.0149e-05 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "10/10 [==============================] - 5s 503ms/step - loss: 2.6464e-05 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "10/10 [==============================] - 5s 496ms/step - loss: 3.6867e-05 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "10/10 [==============================] - 5s 489ms/step - loss: 3.6244e-05 - acc: 1.0000\n",
            "학습 시간 : 995.33초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mDE4V1cCSlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded97068-5f18-4d97-e3ab-58d5f722d470"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=1)\r\n",
        "print(score[0]) # 최종 손실\r\n",
        "print(score[1]) # 최종 정확도"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 1s 55ms/step - loss: 1.6889e-06 - acc: 1.0000\n",
            "1.6888532172742998e-06\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}