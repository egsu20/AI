{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_news",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZbKmhDZ7pCd6FKvixpsSaIe5zJAt_Vt5",
      "authorship_tag": "ABX9TyOegJS3pHZCkI5s5oT4BT/z"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3X6EadwvNC"
      },
      "source": [
        "### Keras RNN으로 BBC 기사 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_IxeQZLw5kx"
      },
      "source": [
        "1. 패키지 수입 및 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouhq20E_wpbB"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer \r\n",
        "from keras.preprocessing.sequence import pad_sequences # 기사 길이를 맞출 때 사용 \r\n",
        "from time import time\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM, Dropout, Embedding\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from sklearn.metrics import confusion_matrix, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0q1LF0Eyumh"
      },
      "source": [
        "# 하이퍼 파라미터 설정\r\n",
        "MY_VOCAB = 5000     # 사용할 단어 수, 제일 많이 사용된 단어\r\n",
        "MY_EMBED = 64       # 임베딩 차원\r\n",
        "MY_HIDDEN = 100     # 뉴런의 개수, LSTM 셀의 규모\r\n",
        "MY_LEN = 200        # 기사의 길이 \r\n",
        "\r\n",
        "MY_SPLIT = 0.8      # 학습용 데이터\r\n",
        "MY_SAMPLE = 123     # 샘플 기사\r\n",
        "MY_EPOCH = 10      # 반복 학습 수. 실험 시간을 줄이기 위해선 낮춰줌. but 정확도 감소\r\n",
        "TRAIN_MODE = 1      # 학습 모드와 평가 모드 선택, 학습된 데이터를 파일로 저장할 때 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1cvbRat0sdw"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYSWahV60uiF",
        "outputId": "d2081a5c-b98c-4032-9a1a-5380b9316c74"
      },
      "source": [
        "# 제외어(stopword) 설정 \r\n",
        "nltk.download('stopwords')\r\n",
        "MY_STOP = set(nltk.corpus.stopwords.words('english')) # 영어와 관련된 제외어를 불러옴\r\n",
        "\r\n",
        "print('영어 제외어')\r\n",
        "print(MY_STOP)\r\n",
        "print('제외어 개수 :', len(MY_STOP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "영어 제외어\n",
            "{\"she's\", 'for', 'ourselves', 'who', 've', 'a', 'which', 'in', 'we', 'does', 'and', \"should've\", \"wasn't\", 'i', 'herself', 'such', 'other', \"you'd\", 'll', 'about', 'on', 'because', 'while', 'can', 'under', 'these', 'y', 'you', 'she', 'own', 'd', \"needn't\", 'below', 'yourself', 'again', 'himself', 'now', 'between', \"haven't\", 'through', 'him', 'won', 'same', 'yourselves', 'have', 'ma', 'but', 'its', 'they', 'over', 'once', \"didn't\", 'how', 'he', 'whom', 'do', 'ours', 'her', 'doing', 'up', 'until', 'not', 'been', 'shouldn', \"isn't\", \"it's\", \"you're\", 'as', 'don', \"mightn't\", 'themselves', 'above', 'didn', 'me', 'our', 'hers', 'ain', 'are', 'had', 'did', 'his', 'hasn', \"weren't\", 'being', 'both', 'than', 'm', 're', 'couldn', 'only', 'with', 'mightn', 'of', 'before', 'o', 'their', 'theirs', 'hadn', 'itself', \"wouldn't\", 'an', 'myself', 'mustn', 'having', 'each', 'at', 'this', 'then', 'too', 'into', \"you'll\", 'when', 'against', 'my', 'if', 'where', 'there', 'the', 'what', 'isn', 'yours', 'aren', 'was', 'few', 'here', 'any', 'nor', 'after', 't', \"hasn't\", 'were', 'wasn', \"mustn't\", 'am', 'off', \"aren't\", 'down', 'that', 's', \"shan't\", 'so', 'more', 'has', \"doesn't\", 'from', 'is', \"that'll\", 'doesn', 'needn', 'be', 'your', 'weren', 'further', 'all', 'no', 'why', 'will', 'to', 'out', 'very', \"don't\", 'just', \"couldn't\", 'haven', 'some', 'shan', \"you've\", 'during', 'most', 'them', 'those', 'by', \"won't\", \"shouldn't\", 'should', 'wouldn', \"hadn't\", 'it', 'or'}\n",
            "제외어 개수 : 179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NviICLLv2xPw"
      },
      "source": [
        "# 데이터 보관 창고\r\n",
        "original = [] # 원본 기사\r\n",
        "articles = [] # 제외어를 삭제한 기사\r\n",
        "labels = [] # 카테고리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ9myFNc3L3_",
        "outputId": "47c0e726-b5c2-4133-a374-9a19b901a2f0"
      },
      "source": [
        "# BBC 파일 읽고 처리\r\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/bbc-text.csv','r') as file:\r\n",
        "    # 컬럼 이름 읽기\r\n",
        "    reader = csv.reader(file)    \r\n",
        "    next(reader) # 컬럼 명 다음의 데이터부터\r\n",
        "\r\n",
        "    # 기사 하나(한 행)씩 처리\r\n",
        "    for row in reader:\r\n",
        "        # 카테고리 저장\r\n",
        "        labels.append(row[0])\r\n",
        "        \r\n",
        "        # 원본 기사 저장\r\n",
        "        original.append(row[1])\r\n",
        "\r\n",
        "        # 제외어 삭제 하기\r\n",
        "        news = row[1]\r\n",
        "        for word in MY_STOP:\r\n",
        "            mask = ' ' + word + ' ' \r\n",
        "            news = news.replace(mask, ' ') # news에 mask를 ' '로 replace한 내용 저장\r\n",
        "        articles.append(news)\r\n",
        "\r\n",
        "print('처리한 기사 수 :', len(articles))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리한 기사 수 : 2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP9142aZBzvZ",
        "outputId": "1a2abf5f-bb3b-4606-972e-9a977789dd37"
      },
      "source": [
        "# 샘플 기사 출력\r\n",
        "print('샘플 기사 원본')\r\n",
        "print(original[MY_SAMPLE]) # MY_SAMPLE번 째 기사 확인\r\n",
        "print(labels[MY_SAMPLE])\r\n",
        "# 알파벳 수 : len(original[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(original[MY_SAMPLE].split())) # 디폴트는 공백"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 원본\n",
            "screensaver tackles spam websites net users are getting the chance to fight back against spam websites  internet portal lycos has made a screensaver that endlessly requests data from sites that sell the goods and services mentioned in spam e-mail. lycos hopes it will make the monthly bandwidth bills of spammers soar by keeping their servers running flat out. the net firm estimates that if enough people sign up and download the tool  spammers could end up paying to send out terabytes of data.   we ve never really solved the big problem of spam which is that its so damn cheap and easy to do   said malte pollmann  spokesman for lycos europe.  in the past we have built up the spam filtering systems for our users   he said   but now we are going to go one step further.    we ve found a way to make it much higher cost for spammers by putting a load on their servers.  by getting thousands of people to download and use the screensaver  lycos hopes to get spamming websites constantly running at almost full capacity. mr pollmann said there was no intention to stop the spam websites working by subjecting them with too much data to cope with. he said the screensaver had been carefully written to ensure that the amount of traffic it generated from each user did not overload the web.  every single user will contribute three to four megabytes per day   he said   about one mp3 file.  but  he said  if enough people sign up spamming websites could be force to pay for gigabytes of traffic every single day. lycos did not want to use e-mail to fight back  said mr pollmann.  that would be fighting one bad thing with another bad thing   he said.  the sites being targeted are those mentioned in spam e-mail messages and which sell the goods and services on offer.  typically these sites are different to those that used to send out spam e-mail and they typically only get a few thousand visitors per day. the list of sites that the screensaver will target is taken from real-time blacklists generated by organisations such as spamcop. to limit the chance of mistakes being made  lycos is using people to ensure that the sites are selling spam goods. as these sites rarely use advertising to offset hosting costs  the burden of high-bandwidth bills could make spam too expensive  said mr pollmann. sites will also slow down under the weight of data requests. early results show that response times of some sites have deteriorated by up to 85%. users do not have to be registered users of lycos to download and use the screensaver. while working  the screensaver shows the websites that are being bothered with requests for data. the screensaver is due to be launched across europe on 1 december and before now has only been trialled in sweden. despite the soft launch  mr pollmann said that the screensaver had been downloaded more than 20 000 times in the last four days.  there s a huge user demand to not only filter spam day-by-day but to do something more   he said  before now users have never had the chance to be a bit more offensive.\n",
            "tech\n",
            "총 단어 수 : 536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlQ86jsvEjCO",
        "outputId": "0953ef72-3310-44c0-82ae-243f20942979"
      },
      "source": [
        "# 제외어 처리 결과\r\n",
        "print('샘플 기사 제외어 삭제본')\r\n",
        "print(articles[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(articles[MY_SAMPLE].split())) # 디폴트는 공백"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 제외어 삭제본\n",
            "screensaver tackles spam websites net users getting chance fight back spam websites  internet portal lycos made screensaver endlessly requests data sites sell goods services mentioned spam e-mail. lycos hopes make monthly bandwidth bills spammers soar keeping servers running flat out. net firm estimates enough people sign download tool  spammers could end paying send terabytes data.   never really solved big problem spam damn cheap easy   said malte pollmann  spokesman lycos europe.  past built spam filtering systems users   said   going go one step further.    found way make much higher cost spammers putting load servers.  getting thousands people download use screensaver  lycos hopes get spamming websites constantly running almost full capacity. mr pollmann said intention stop spam websites working subjecting much data cope with. said screensaver carefully written ensure amount traffic generated user overload web.  every single user contribute three four megabytes per day   said   one mp3 file.   said  enough people sign spamming websites could force pay gigabytes traffic every single day. lycos want use e-mail fight back  said mr pollmann.  would fighting one bad thing another bad thing   said.  sites targeted mentioned spam e-mail messages sell goods services offer.  typically sites different used send spam e-mail typically get thousand visitors per day. list sites screensaver target taken real-time blacklists generated organisations spamcop. limit chance mistakes made  lycos using people ensure sites selling spam goods. sites rarely use advertising offset hosting costs  burden high-bandwidth bills could make spam expensive  said mr pollmann. sites also slow weight data requests. early results show response times sites deteriorated 85%. users registered users lycos download use screensaver. working  screensaver shows websites bothered requests data. screensaver due launched across europe 1 december trialled sweden. despite soft launch  mr pollmann said screensaver downloaded 20 000 times last four days.  huge user demand filter spam day-by-day something   said  users never chance bit offensive.\n",
            "총 단어 수 : 303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnwEWk31GR3F",
        "outputId": "29ac02ca-dbe2-4c5a-e2a0-6ac542f2e4e9"
      },
      "source": [
        "# Tokenizer 처리\r\n",
        "A_token = Tokenizer(num_words=MY_VOCAB, # 데이터(단어)를 몇 개 쓸 것인가\r\n",
        "                    oov_token='OOV') # 'Out Of Vocab'. \r\n",
        "                                     # 자주 쓰이는 5000개 단어보다 덜 쓰이는(어려운) 단어 처리를 어떻게 할 것 인가\r\n",
        "                                     # 그 단어를 OOV로 설정하겠다\r\n",
        "\r\n",
        "A_token.fit_on_texts(articles) # articles의 단어를 숫자로 바꾸기 위한 준비과정(hash function을 만듦)\r\n",
        "A_tokenized = A_token.texts_to_sequences(articles) # 토큰 결과. 실제로 영어 단어를 숫자로 바꾸어 줌\r\n",
        "# hash function\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "# 숫자 -> 단어\r\n",
        "print(A_token.sequences_to_texts([[1]])) # 1이라는 숫자를 단어로 역전환 => OOV(누락된 단어)가 출력됨.\r\n",
        "# 단어 -> 숫자\r\n",
        "print(A_token.texts_to_sequences(['the'])) # 'the' 단어가 어떤 숫자로 변환? 인기도와 관계 x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['OOV']\n",
            "[[1113]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwgM57G4INO9",
        "outputId": "bc326b33-052e-4f7a-bdd2-3381387b971a"
      },
      "source": [
        "# Token 처리 결과 출력\r\n",
        "sample = A_tokenized[MY_SAMPLE]\r\n",
        "print(sample) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3171, 1, 816, 878, 115, 136, 382, 347, 716, 28, 816, 878, 228, 1, 3172, 27, 3171, 1, 4869, 204, 569, 733, 1771, 126, 4025, 816, 260, 395, 3172, 700, 21, 1649, 3629, 2848, 2606, 1, 2324, 2550, 453, 2918, 570, 115, 63, 2290, 381, 7, 1161, 780, 1859, 2606, 11, 92, 1571, 1051, 1, 204, 281, 154, 1, 138, 364, 816, 1, 2225, 847, 2, 1, 1, 178, 3172, 139, 255, 1109, 816, 1, 726, 136, 2, 52, 60, 10, 818, 3790, 195, 41, 21, 56, 495, 245, 2606, 1362, 1, 2550, 382, 1021, 7, 780, 70, 3171, 3172, 700, 23, 1, 878, 3993, 453, 343, 322, 1393, 3, 1, 2, 3427, 582, 816, 878, 297, 1, 56, 204, 2295, 2403, 2, 3171, 2708, 1069, 660, 812, 1287, 3884, 1539, 1, 466, 224, 504, 1539, 1, 31, 96, 1, 681, 111, 2, 10, 1898, 912, 2, 381, 7, 1161, 1, 878, 11, 722, 256, 1, 1287, 224, 504, 111, 3172, 79, 70, 260, 395, 716, 28, 2, 3, 1, 4, 1604, 10, 823, 455, 158, 823, 455, 2, 569, 2179, 4025, 816, 260, 395, 891, 733, 1771, 126, 221, 3677, 569, 316, 86, 1051, 816, 260, 395, 3677, 23, 1, 1452, 681, 111, 415, 569, 3171, 760, 367, 189, 14, 1, 3884, 1595, 1, 1374, 347, 3753, 27, 3172, 201, 7, 660, 569, 848, 816, 1771, 569, 3258, 70, 2068, 4063, 4054, 416, 3791, 77, 3629, 2848, 11, 21, 816, 1731, 2, 3, 1, 569, 6, 1430, 4185, 204, 4869, 251, 664, 65, 910, 231, 569, 1, 3261, 136, 2819, 136, 3172, 780, 70, 3171, 297, 3171, 571, 878, 1, 4869, 204, 3171, 269, 633, 383, 139, 35, 233, 1, 2643, 193, 4459, 610, 3, 1, 2, 3171, 1966, 264, 32, 231, 12, 96, 276, 430, 1539, 379, 1, 816, 111, 2984, 111, 323, 2, 136, 281, 347, 651, 4064]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HnpFl9P416",
        "outputId": "15051c94-6ecb-40c2-d6f5-b6579ba2d0d9"
      },
      "source": [
        "# 기사 통계 내기\r\n",
        "# 제외어 제외, 토큰 처리 후.\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "print('제일 짧은 기사 :', shortest)\r\n",
        "\r\n",
        "print('총 단어 수 :', len(A_token.word_counts)) # A_token.word_counts : 단어가 사용된 횟수 출력\r\n",
        "# 총 단어 수 n개 중 MY_VOCAB개만 기계학습에 사용"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 2280\n",
            "제일 짧은 기사 : 50\n",
            "총 단어 수 : 29698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cN3WjeOSIWr",
        "outputId": "c9226cd4-9b2a-4bb1-8da0-33ca43624af8"
      },
      "source": [
        "# 기사 길이 맞추기 -> 모든 기사의 길이를 맞춰줌\r\n",
        "A_tokenized = pad_sequences(A_tokenized,\r\n",
        "                            maxlen=MY_LEN,\r\n",
        "                            padding='post', # 200 단어보다 짧은 기사는 0으로 뒷부분 패딩 처리\r\n",
        "                            truncating='post') # 200 단어보다 긴 기사는 뒷 부분 삭제 \r\n",
        "\r\n",
        "# 기사 길이 확인\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "print('제일 짧은 기사 :', shortest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 200\n",
            "제일 짧은 기사 : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkwWlbgMUbLm",
        "outputId": "cb13c35b-fa54-4b17-9262-caae7055765b"
      },
      "source": [
        "# 라벨 tokenization\r\n",
        "C_token = Tokenizer()\r\n",
        "C_token.fit_on_texts(labels) # 변환 준비 과정. hash function을 만듦\r\n",
        "C_tokenized = C_token.texts_to_sequences(labels) # 카테고리를 숫자로 변환한 결과\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(C_token.word_index)\r\n",
        "print(C_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2EqkdBCWvvL",
        "outputId": "70c60807-2027-4b18-fc13-dc1903c22532"
      },
      "source": [
        "# 데이터 4분할\r\n",
        "C_tokenized = np.array(C_tokenized) # list를 numpy로 변환\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized, #  입력 data\r\n",
        "                                                    C_tokenized, # 출력 data\r\n",
        "                                                    train_size=MY_SPLIT,\r\n",
        "                                                    shuffle=False)\r\n",
        "\r\n",
        "# 데이터 모양 확인 \r\n",
        "print('학습용 입력 데이터 모양 :', X_train.shape)\r\n",
        "print('학습용 출력 데이터 모양 :', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터 모양 :', X_test.shape)\r\n",
        "print('평가용 출력 데이터 모양 :', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 모양 : (1780, 200)\n",
            "학습용 출력 데이터 모양 : (1780, 1)\n",
            "평가용 입력 데이터 모양 : (445, 200)\n",
            "평가용 출력 데이터 모양 : (445, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9tAGX52k5uH"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag_-Dgfnk_c5",
        "outputId": "25501290-6b0b-4d58-c82c-782c6a0b2334"
      },
      "source": [
        "# RNN 구현\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(input_dim=MY_VOCAB, # MY_VOCAB개를\r\n",
        "                    output_dim=MY_EMBED)) # MY_EMBED개로\r\n",
        "\r\n",
        "model.add(Dropout(rate=0.5)) # Dropout : 임의의 뉴런의 출력을 일부로 0으로 만드는 작업\r\n",
        "                             # 과적합(overfitting) 방지하기 위해 뉴런 몇 개를 죽임\r\n",
        "                             # 과적합은 새로운 문제해결에 해가 될 수 있음\r\n",
        "\r\n",
        "model.add(Bidirectional(LSTM(units=MY_HIDDEN)))\r\n",
        "\r\n",
        "model.add(Dense(units=6,\r\n",
        "                activation='softmax'))\r\n",
        "\r\n",
        "print('RNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN 요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200)               132000    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 453,206\n",
            "Trainable params: 453,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHEtRrkZqwE3"
      },
      "source": [
        "4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnRdeBLqzO3",
        "outputId": "bbf38f16-670c-4752-c29d-c403358f54cc"
      },
      "source": [
        "# RNN 학습\r\n",
        "model.compile(optimizer='adam',\r\n",
        "              loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['acc'])\r\n",
        "\r\n",
        "print(' 학습 시작')\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x=X_train,\r\n",
        "          y=Y_train,\r\n",
        "          epochs=MY_EPOCH,\r\n",
        "          verbose=1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습 시간 : {:.2f}'.format(end - begin))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 학습 시작\n",
            "Epoch 1/10\n",
            "56/56 [==============================] - 11s 25ms/step - loss: 1.6751 - acc: 0.2364\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.2493 - acc: 0.5235\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.6446 - acc: 0.8225\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.5070 - acc: 0.8320\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1724 - acc: 0.9622\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.1671 - acc: 0.9618\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0578 - acc: 0.9925\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0304 - acc: 0.9953\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0444 - acc: 0.9910\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0134 - acc: 0.9997\n",
            "학습 시간 : 22.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S83BcC2Ys03B"
      },
      "source": [
        "5. 인공 신경망 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KBwEDtqsziO",
        "outputId": "e07c9e60-abfc-476a-e307-a1d3af494c4c"
      },
      "source": [
        "# RNN 평가\r\n",
        "score = model.evaluate(X_test,\r\n",
        "                       Y_test,\r\n",
        "                       verbose=0)\r\n",
        "\r\n",
        "print('최종 손실값 : {:.2f}'.format(score[0]))\r\n",
        "print('최종 정확도 : {:.2f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 손실값 : 0.22\n",
            "최종 정확도 : 0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ilmr-MeJEF_"
      },
      "source": [
        "6. 인공 신경망 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2IXW-VUJHPB",
        "outputId": "68f713bd-aa0a-47c3-b947-159acc52d795"
      },
      "source": [
        "# RNN 예측\r\n",
        "pred = model.predict(X_test) # 0번째 output은 대부분 사용하지 않음\r\n",
        "pred = pred.argmax(axis=1) # 제일 큰 값이 몇 번째 있나. 몇 번 축을 쓸 지 정해줘야함. 열 사용\r\n",
        "print(pred)\r\n",
        "print(Y_test.flatten()) # Y_test는 2차원 데이터로 한 줄로 출력됨"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5 4 3 1 1 4 3 4 5 5 3 3 2 5 1 5 5 2 1 3 4 2 1 5 4 3 3 1 1 3 2 2 2 2 5 2 3\n",
            " 3 4 4 5 3 5 2 3 1 1 5 4 2 4 1 2 2 3 1 1 3 3 5 5 3 2 3 3 2 4 3 3 3 3 3 5 5\n",
            " 4 3 1 4 1 4 5 1 1 5 4 5 4 1 4 1 1 5 5 2 5 5 3 2 1 4 4 3 2 1 2 4 1 3 5 1 1\n",
            " 2 3 4 4 2 2 1 3 5 1 1 3 5 4 1 5 2 3 1 3 4 5 1 3 2 5 3 5 3 1 3 3 2 3 2 4 1\n",
            " 2 5 2 1 1 4 4 3 4 3 3 1 1 1 2 4 5 3 1 2 1 2 4 2 2 2 2 1 1 1 2 2 5 2 2 2 1\n",
            " 1 1 4 4 1 1 1 2 5 4 4 4 3 2 2 4 2 4 1 1 3 3 3 1 1 3 3 4 2 1 1 1 1 2 1 2 2\n",
            " 3 2 1 3 1 4 4 1 4 2 5 2 1 2 4 4 3 5 2 5 2 4 3 5 3 5 5 4 3 4 4 2 3 1 5 2 3\n",
            " 5 2 4 1 4 3 1 3 2 3 3 2 2 2 4 3 2 3 2 5 3 1 3 3 1 5 4 4 2 4 1 4 3 2 1 4 4\n",
            " 4 1 5 1 3 2 3 3 5 4 2 4 1 5 5 1 2 5 4 4 1 5 2 3 3 3 4 4 2 3 2 4 3 5 5 4 2\n",
            " 4 5 4 4 1 3 1 1 3 5 5 2 3 3 1 2 2 4 2 4 4 1 2 3 1 2 2 1 4 1 4 5 1 1 5 2 4\n",
            " 1 1 3 4 3 3 1 1 3 2 4 4 4 2 1 5 5 4 2 3 4 1 1 4 4 3 2 1 5 4 1 3 4 1 2 2 2\n",
            " 1 1 4 1 3 4 2 2 1 2 3 2 2 5 3 4 3 4 5 3 4 5 1 3 5 4 4 2 4 5 4 1 4 2 3 5 3\n",
            " 1]\n",
            "[5 4 3 1 1 4 2 4 5 5 3 3 2 5 1 5 5 2 1 3 4 2 1 5 4 3 3 1 1 2 2 2 2 2 5 2 3\n",
            " 3 4 4 5 3 5 2 3 1 1 2 4 2 4 1 2 2 3 1 1 3 3 5 5 3 2 3 3 2 4 3 3 3 3 3 5 5\n",
            " 4 3 1 3 1 4 1 1 1 5 4 5 4 1 4 1 1 5 5 2 5 5 3 2 1 4 4 3 2 1 2 5 1 3 5 1 1\n",
            " 2 3 4 4 2 2 1 3 5 1 1 3 5 4 1 5 2 3 1 3 4 5 1 3 2 5 3 5 3 1 3 2 2 3 2 4 1\n",
            " 2 5 2 1 1 5 4 3 4 3 3 1 1 1 2 4 5 2 1 2 1 2 4 2 2 2 2 1 1 1 2 2 5 2 2 2 1\n",
            " 1 1 4 2 1 1 1 2 5 4 4 4 3 2 2 4 2 4 1 1 3 3 3 1 1 3 3 4 2 1 1 1 1 2 1 2 2\n",
            " 2 2 1 3 1 4 4 1 4 2 5 2 1 2 4 4 3 5 2 5 2 4 3 5 3 5 5 4 2 4 4 2 3 1 5 2 3\n",
            " 5 2 4 1 4 3 1 3 2 3 3 2 2 2 4 3 2 3 2 5 3 1 3 3 1 5 4 4 2 4 1 2 2 3 1 4 4\n",
            " 4 1 5 1 3 2 3 3 5 4 2 4 1 5 5 1 2 5 4 4 1 5 2 3 3 3 4 4 2 3 2 3 3 5 1 4 2\n",
            " 4 5 4 4 1 3 1 1 3 5 5 2 3 3 1 2 2 4 2 4 4 1 2 3 1 2 2 1 4 1 4 5 1 1 5 2 4\n",
            " 1 1 3 4 2 3 1 1 3 5 4 4 4 2 1 5 5 4 2 3 4 1 1 4 4 3 2 1 5 5 1 5 4 4 2 2 2\n",
            " 1 1 4 1 2 4 2 2 1 2 3 2 2 4 2 4 3 4 5 3 4 5 1 3 5 2 4 2 4 5 4 1 2 2 3 5 3\n",
            " 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9MwW9aWMxvj",
        "outputId": "ad96c20e-b595-4a6d-a7ce-99aa85452bba"
      },
      "source": [
        "# 혼동 행렬 출력\r\n",
        "print('혼동 행렬')\r\n",
        "print(confusion_matrix(y_true=Y_test,\r\n",
        "                       y_pred=pred)) # 대각선에 있는 값은 맞은 것.\r\n",
        "                                     # 혼동 행렬에서 행 : 예측, 열 : 정답\r\n",
        "                                     # ex) [0][0] : 1번을 예측해서 맞은 것 (사실상 [1][1])\r\n",
        "                                     # ex) [2][0] : 1번 답을 3번으로 예측한 것"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "혼동 행렬\n",
            "[[99  0  0  0  2]\n",
            " [ 0 91 10  4  1]\n",
            " [ 0  1 83  2  0]\n",
            " [ 1  0  0 84  1]\n",
            " [ 0  1  1  3 61]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEPrYunEODg0",
        "outputId": "73e315ac-0a2f-4bc7-b397-2ab29fdb24c8"
      },
      "source": [
        "# 실제 기사로 분류\r\n",
        "news = [\"'Paul Pogba's second-half volley was enough to give Manchester United victory at Burnley and send them three points clear at the top of the Premier League. United dominated a contest in which Burnley failed to register a single shot on target until stoppage time. But they were struggling to make a breakthrough until Marcus Rashford picked Pogba out with an excellent cross to the edge of the area. The Frenchman's connection was perfect, although it took a deflection off Matthew Lowton to ensure the ball went past Nick Pope and into the Burnley net. Although Burnley had three decent chances in a frantic ending, United secured the win to head the table after 17 rounds of matches.'\"]\r\n",
        "\r\n",
        "news = A_token.texts_to_sequences(news)\r\n",
        "print(news)\r\n",
        "\r\n",
        "news = pad_sequences(news, #처리할 데이터\r\n",
        "                     maxlen=MY_LEN,\r\n",
        "                     padding='post', # 뒷부분 0으로 패딩처리\r\n",
        "                     truncating='post')\r\n",
        "\r\n",
        "# print(news)\r\n",
        "pred = model.predict(news) # 확률값\r\n",
        "pred = pred.argmax(axis=1)\r\n",
        "print('RNN 추측값 :', pred)\r\n",
        "\r\n",
        "# {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 1, 64, 104, 4061, 3917, 381, 476, 209, 658, 190, 434, 3496, 4402, 2002, 1051, 598, 31, 550, 373, 3496, 1113, 66, 1635, 1113, 2568, 463, 190, 1446, 1385, 1751, 619, 1, 4402, 535, 476, 2991, 1385, 504, 811, 584, 760, 1, 1, 14, 1, 1, 1, 1848, 476, 21, 1385, 2533, 1, 1, 1, 1309, 1, 570, 2403, 1, 2560, 872, 476, 1113, 2177, 1635, 1113, 826, 1113, 1, 1581, 3917, 2598, 238, 197, 169, 1385, 1, 813, 2820, 1, 476, 660, 1113, 692, 293, 255, 2852, 1, 2002, 1, 1113, 4402, 115, 238, 4402, 3842, 31, 3291, 1368, 619, 1385, 1, 3342, 190, 2626, 1113, 58, 476, 392, 1113, 2366, 1, 579, 1, 1635, 1251, 1]]\n",
            "RNN 추측값 : [1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}