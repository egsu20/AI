{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9_news",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ZbKmhDZ7pCd6FKvixpsSaIe5zJAt_Vt5",
      "authorship_tag": "ABX9TyPRJHYVSD/i3Q30V18GT7Q/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH3X6EadwvNC"
      },
      "source": [
        "### Keras RNN으로 BBC 기사 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_IxeQZLw5kx"
      },
      "source": [
        "1. 패키지 수입 및 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ouhq20E_wpbB"
      },
      "source": [
        "# 패키지 수입\r\n",
        "import csv\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer \r\n",
        "from keras.preprocessing.sequence import pad_sequences # 기사 길이를 맞출 때 사용 \r\n",
        "from time import time\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import LSTM, Dropout, Embedding\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from sklearn.metrics import confusion_matrix, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0q1LF0Eyumh"
      },
      "source": [
        "# 하이퍼 파라미터 설정\r\n",
        "MY_VOCAB = 5000     # 사용할 단어 수, 제일 많이 사용된 단어\r\n",
        "MY_EMBED = 64       # 임베딩 차원\r\n",
        "MY_HIDDEN = 100     # 뉴런의 개수, LSTM 셀의 규모\r\n",
        "MY_LEN = 200        # 기사의 길이\r\n",
        "\r\n",
        "MY_SPLIT = 0.8      # 학습용 데이터\r\n",
        "MY_SAMPLE = 123     # 샘플 기사\r\n",
        "MY_EPOCH = 100      # 반복 학습 수\r\n",
        "TRAIN_MODE = 1      # 학습 모드와 평가 모드 선택, 학습된 데이터를 파일로 저장할 때 사용"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1cvbRat0sdw"
      },
      "source": [
        "2. 데이터 처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYSWahV60uiF",
        "outputId": "513f2bb2-ef60-4312-abc2-40aa2b9303c2"
      },
      "source": [
        "# 제외어(stopword) 설정 \r\n",
        "nltk.download('stopwords')\r\n",
        "MY_STOP = set(nltk.corpus.stopwords.words('english')) # 영어와 관련된 제외어를 불러옴\r\n",
        "\r\n",
        "print('영어 제외어')\r\n",
        "print(MY_STOP)\r\n",
        "print('제외어 개수 :', len(MY_STOP))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "영어 제외어\n",
            "{'how', 'which', 'are', 'has', 'then', 'when', 'those', \"hadn't\", 'doing', 'having', 'at', 'its', 'what', 'ain', 'about', 'needn', 'ourselves', 'that', 'nor', 'until', 'wasn', 'y', 'between', \"needn't\", 'now', \"don't\", 'have', 'with', 'haven', 'in', 'was', 'do', \"didn't\", 'and', 'yourself', 'me', 'herself', 'because', 'it', \"doesn't\", 'won', 'myself', 'ours', 'against', \"won't\", \"she's\", 'been', 'above', 'both', 'a', \"aren't\", 'isn', 'by', \"you'd\", \"you'll\", 'some', 'you', 'were', 'hers', 'theirs', 'wouldn', 'himself', 'for', 'shouldn', 'after', 'up', 'again', 'through', 'all', 'm', \"weren't\", 'whom', 'd', 'they', 'once', 'further', 'the', 'here', 'o', 'there', 'any', 'couldn', 'below', 'an', 'than', 'few', 'him', 'during', 'where', 'aren', 'who', 'if', 'on', 'ma', 'or', 'hadn', 'i', 'each', 'same', \"mustn't\", 'yourselves', 'my', 'did', \"haven't\", 'itself', 'am', \"couldn't\", \"isn't\", 'he', 'll', 'had', 'will', 'doesn', 'them', 'while', 'down', 'into', 'not', 'too', \"shouldn't\", 're', 'only', 'to', 'themselves', 'don', \"should've\", 'from', 'these', 'no', 'our', 'does', 'being', 'is', 'yours', 'under', 'her', 'mustn', 'mightn', \"it's\", 'so', 'weren', 'other', 'this', 'very', 'we', \"that'll\", 'she', 'such', 'didn', 'their', \"you've\", \"you're\", 'most', 'should', 'over', 'his', 'can', 'your', \"hasn't\", 'hasn', 'shan', 've', 'just', 'before', 'own', 'but', \"mightn't\", 'out', \"wouldn't\", 't', \"shan't\", 'why', \"wasn't\", 's', 'be', 'of', 'as', 'more', 'off'}\n",
            "제외어 개수 : 179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NviICLLv2xPw"
      },
      "source": [
        "# 데이터 보관 창고\r\n",
        "original = [] # 원본 기사\r\n",
        "articles = [] # 제외어를 삭제한 기사\r\n",
        "labels = [] # 카테고리"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ9myFNc3L3_",
        "outputId": "8fabbcf8-ed9e-4924-8b7c-9e728400286d"
      },
      "source": [
        "# BBC 파일 읽고 처리\r\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/data/bbc-text.csv','r') as file:\r\n",
        "    # 컬럼 이름 읽기\r\n",
        "    reader = csv.reader(file)    \r\n",
        "    next(reader) # 컬럼 명 다음의 데이터부터\r\n",
        "\r\n",
        "    # 기사 하나(한 행)씩 처리\r\n",
        "    for row in reader:\r\n",
        "        # 카테고리 저장\r\n",
        "        labels.append(row[0])\r\n",
        "        \r\n",
        "        # 원본 기사 저장\r\n",
        "        original.append(row[1])\r\n",
        "\r\n",
        "        # 제외어 삭제 하기\r\n",
        "        news = row[1]\r\n",
        "        for word in MY_STOP:\r\n",
        "            mask = ' ' + word + ' ' \r\n",
        "            news = news.replace(mask, ' ') # news에 mask를 ' '로 replace한 내용 저장\r\n",
        "        articles.append(news)\r\n",
        "\r\n",
        "print('처리한 기사 수 :', len(articles))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "처리한 기사 수 : 2225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LP9142aZBzvZ",
        "outputId": "59d6e520-88fd-4c0c-c142-98045c1c46d1"
      },
      "source": [
        "# 샘플 기사 출력\r\n",
        "print('샘플 기사 원본')\r\n",
        "print(original[MY_SAMPLE]) # MY_SAMPLE번 째 기사 확인\r\n",
        "print(labels[MY_SAMPLE])\r\n",
        "# 알파벳 수 : len(original[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(original[MY_SAMPLE].split())) # 디폴트는 공백"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 원본\n",
            "screensaver tackles spam websites net users are getting the chance to fight back against spam websites  internet portal lycos has made a screensaver that endlessly requests data from sites that sell the goods and services mentioned in spam e-mail. lycos hopes it will make the monthly bandwidth bills of spammers soar by keeping their servers running flat out. the net firm estimates that if enough people sign up and download the tool  spammers could end up paying to send out terabytes of data.   we ve never really solved the big problem of spam which is that its so damn cheap and easy to do   said malte pollmann  spokesman for lycos europe.  in the past we have built up the spam filtering systems for our users   he said   but now we are going to go one step further.    we ve found a way to make it much higher cost for spammers by putting a load on their servers.  by getting thousands of people to download and use the screensaver  lycos hopes to get spamming websites constantly running at almost full capacity. mr pollmann said there was no intention to stop the spam websites working by subjecting them with too much data to cope with. he said the screensaver had been carefully written to ensure that the amount of traffic it generated from each user did not overload the web.  every single user will contribute three to four megabytes per day   he said   about one mp3 file.  but  he said  if enough people sign up spamming websites could be force to pay for gigabytes of traffic every single day. lycos did not want to use e-mail to fight back  said mr pollmann.  that would be fighting one bad thing with another bad thing   he said.  the sites being targeted are those mentioned in spam e-mail messages and which sell the goods and services on offer.  typically these sites are different to those that used to send out spam e-mail and they typically only get a few thousand visitors per day. the list of sites that the screensaver will target is taken from real-time blacklists generated by organisations such as spamcop. to limit the chance of mistakes being made  lycos is using people to ensure that the sites are selling spam goods. as these sites rarely use advertising to offset hosting costs  the burden of high-bandwidth bills could make spam too expensive  said mr pollmann. sites will also slow down under the weight of data requests. early results show that response times of some sites have deteriorated by up to 85%. users do not have to be registered users of lycos to download and use the screensaver. while working  the screensaver shows the websites that are being bothered with requests for data. the screensaver is due to be launched across europe on 1 december and before now has only been trialled in sweden. despite the soft launch  mr pollmann said that the screensaver had been downloaded more than 20 000 times in the last four days.  there s a huge user demand to not only filter spam day-by-day but to do something more   he said  before now users have never had the chance to be a bit more offensive.\n",
            "tech\n",
            "총 단어 수 : 536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlQ86jsvEjCO",
        "outputId": "4516a6c7-a748-4b14-ec7b-835365476620"
      },
      "source": [
        "# 제외어 처리 결과\r\n",
        "print('샘플 기사 제외어 삭제본')\r\n",
        "print(articles[MY_SAMPLE])\r\n",
        "print('총 단어 수 :', len(articles[MY_SAMPLE].split())) # 디폴트는 공백"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플 기사 제외어 삭제본\n",
            "screensaver tackles spam websites net users getting chance fight back spam websites  internet portal lycos made screensaver endlessly requests data sites sell goods services mentioned spam e-mail. lycos hopes make monthly bandwidth bills spammers soar keeping servers running flat out. net firm estimates enough people sign download tool  spammers could end paying send terabytes data.   never really solved big problem spam damn cheap easy   said malte pollmann  spokesman lycos europe.  past built spam filtering systems users   said   going go one step further.    found way make much higher cost spammers putting load servers.  getting thousands people download use screensaver  lycos hopes get spamming websites constantly running almost full capacity. mr pollmann said intention stop spam websites working subjecting much data cope with. said screensaver carefully written ensure amount traffic generated user overload web.  every single user contribute three four megabytes per day   said   one mp3 file.   said  enough people sign spamming websites could force pay gigabytes traffic every single day. lycos want use e-mail fight back  said mr pollmann.  would fighting one bad thing another bad thing   said.  sites targeted mentioned spam e-mail messages sell goods services offer.  typically sites different used send spam e-mail typically get thousand visitors per day. list sites screensaver target taken real-time blacklists generated organisations spamcop. limit chance mistakes made  lycos using people ensure sites selling spam goods. sites rarely use advertising offset hosting costs  burden high-bandwidth bills could make spam expensive  said mr pollmann. sites also slow weight data requests. early results show response times sites deteriorated 85%. users registered users lycos download use screensaver. working  screensaver shows websites bothered requests data. screensaver due launched across europe 1 december trialled sweden. despite soft launch  mr pollmann said screensaver downloaded 20 000 times last four days.  huge user demand filter spam day-by-day something   said  users never chance bit offensive.\n",
            "총 단어 수 : 303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnwEWk31GR3F",
        "outputId": "4ad8365c-5527-42fa-d21a-726c816024cb"
      },
      "source": [
        "# Tokenizer 처리\r\n",
        "A_token = Tokenizer(num_words=MY_VOCAB, # 데이터(단어) 몇 개 가지고 있나\r\n",
        "                    oov_token='OOV') # 'Out Of Vocab'. \r\n",
        "                                     # 자주 쓰이는 5000개 단어보다 덜 쓰이는(어려운) 단어 처리를 어떻게 할 것 인가\r\n",
        "                                     # 그 단어를 OOV로 설정하겠다\r\n",
        "\r\n",
        "A_token.fit_on_texts(articles) # articles의 단어를 숫자로 바꾸기 위한 준비과정\r\n",
        "A_tokenized = A_token.texts_to_sequences(articles) # 토큰 결과. 실제로 영어 단어를 숫자로 바꾸어 줌\r\n",
        "# hash function\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "# 숫자 -> 단어\r\n",
        "print(A_token.sequences_to_texts([[1]])) # 1이라는 숫자를 단어로 역전환 => OOV(누락된 단어)가 출력됨.\r\n",
        "# 단어 -> 숫자\r\n",
        "print(A_token.texts_to_sequences(['the'])) # 'the' 단어가 어떤 숫자로 변환? 인기도와 관계 x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['OOV']\n",
            "[[1230]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwgM57G4INO9",
        "outputId": "015f77a5-053d-4c33-b30e-f5af0809a563"
      },
      "source": [
        "# Token 처리 결과 출력\r\n",
        "sample = A_tokenized[MY_SAMPLE]\r\n",
        "print(sample) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3171, 1, 816, 878, 115, 136, 382, 347, 716, 28, 816, 878, 228, 1, 3172, 27, 3171, 1, 4869, 203, 569, 733, 1772, 126, 4024, 816, 260, 395, 3172, 700, 21, 1649, 3629, 2848, 2606, 1, 2325, 2550, 453, 2918, 570, 115, 63, 2291, 381, 7, 1160, 780, 1860, 2606, 11, 92, 1572, 1051, 1, 203, 281, 154, 1, 138, 364, 816, 1, 2225, 847, 2, 1, 1, 178, 3172, 139, 255, 1109, 816, 1, 726, 136, 2, 52, 60, 10, 818, 3790, 195, 41, 21, 56, 494, 245, 2606, 1363, 1, 2550, 382, 1021, 7, 780, 70, 3171, 3172, 700, 23, 1, 878, 3992, 453, 343, 322, 1394, 3, 1, 2, 3427, 583, 816, 878, 297, 1, 56, 203, 2296, 2403, 2, 3171, 2708, 1069, 660, 812, 1287, 3883, 1540, 1, 466, 224, 504, 1540, 1, 31, 96, 1, 681, 111, 2, 10, 1898, 912, 2, 381, 7, 1160, 1, 878, 11, 722, 256, 1, 1287, 224, 504, 111, 3172, 79, 70, 260, 395, 716, 28, 2, 3, 1, 4, 1605, 10, 823, 455, 158, 823, 455, 2, 569, 2179, 4024, 816, 260, 395, 891, 733, 1772, 126, 221, 3677, 569, 316, 86, 1051, 816, 260, 395, 3677, 23, 1, 1454, 681, 111, 415, 569, 3171, 760, 367, 189, 14, 1, 3883, 1596, 1, 1375, 347, 3753, 27, 3172, 200, 7, 660, 569, 848, 816, 1772, 569, 3258, 70, 2068, 4062, 4053, 416, 3791, 77, 3629, 2848, 11, 21, 816, 1731, 2, 3, 1, 569, 6, 1431, 4184, 203, 4869, 251, 664, 65, 910, 231, 569, 1, 3261, 136, 2819, 136, 3172, 780, 70, 3171, 297, 3171, 571, 878, 1, 4869, 203, 3171, 269, 633, 383, 139, 35, 233, 1, 2643, 193, 4458, 610, 3, 1, 2, 3171, 1966, 264, 32, 231, 12, 96, 276, 430, 1540, 379, 1, 816, 111, 2984, 111, 323, 2, 136, 281, 347, 651, 4063]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HnpFl9P416",
        "outputId": "36bd5e3a-5b95-4f17-9d1d-5e01a73e74f0"
      },
      "source": [
        "# 기사 통계 내기\r\n",
        "# 제외어 제외, 토큰 처리 후.\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "print('제일 짧은 기사 :', shortest)\r\n",
        "\r\n",
        "print('총 단어 수 :', len(A_token.word_counts)) # A_token.word_counts : 단어가 사용된 횟수 출력\r\n",
        "# 총 단어 수 n개 중 MY_VOCAB개만 기계학습에 사용"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 2280\n",
            "제일 짧은 기사 : 50\n",
            "총 단어 수 : 29698\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cN3WjeOSIWr",
        "outputId": "6a772a99-b955-469a-93f0-ae1bdee8767e"
      },
      "source": [
        "# 기사 길이 맞추기 -> 모든 기사의 길이를 맞춰줌\r\n",
        "A_tokenized = pad_sequences(A_tokenized,\r\n",
        "                            maxlen=MY_LEN,\r\n",
        "                            padding='post', # 200 단어보다 짧은 기사는 0으로 뒷부분 패딩 처리\r\n",
        "                            truncating='post') # 200 단어보다 긴 기사는 뒷 부분 삭제 \r\n",
        "\r\n",
        "# 기사 길이 확인\r\n",
        "longest = max([len(x) for x in A_tokenized])\r\n",
        "print('제일 긴 기사 :', longest)\r\n",
        "\r\n",
        "shortest = min([len(x) for x in A_tokenized])\r\n",
        "print('제일 짧은 기사 :', shortest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "제일 긴 기사 : 200\n",
            "제일 짧은 기사 : 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkwWlbgMUbLm",
        "outputId": "fc908ae3-599f-4837-b83c-1b6c57d163e4"
      },
      "source": [
        "# 라벨 tokenization\r\n",
        "C_token = Tokenizer()\r\n",
        "C_token.fit_on_texts(labels) # 변환 준비 과정\r\n",
        "C_tokenized = C_token.texts_to_sequences(labels) # 카테고리를 숫자로 변환한 결과\r\n",
        "\r\n",
        "# 전환의 예\r\n",
        "print(C_token.word_index)\r\n",
        "print(C_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
            "[[4], [2], [1], [1], [5], [3], [3], [1], [1], [5], [5], [2], [2], [3], [1], [2], [3], [1], [2], [4], [4], [4], [1], [1], [4], [1], [5], [4], [3], [5], [3], [4], [5], [5], [2], [3], [4], [5], [3], [2], [3], [1], [2], [1], [4], [5], [3], [3], [3], [2], [1], [3], [2], [2], [1], [3], [2], [1], [1], [2], [2], [1], [2], [1], [2], [4], [2], [5], [4], [2], [3], [2], [3], [1], [2], [4], [2], [1], [1], [2], [2], [1], [3], [2], [5], [3], [3], [2], [5], [2], [1], [1], [3], [1], [3], [1], [2], [1], [2], [5], [5], [1], [2], [3], [3], [4], [1], [5], [1], [4], [2], [5], [1], [5], [1], [5], [5], [3], [1], [1], [5], [3], [2], [4], [2], [2], [4], [1], [3], [1], [4], [5], [1], [2], [2], [4], [5], [4], [1], [2], [2], [2], [4], [1], [4], [2], [1], [5], [1], [4], [1], [4], [3], [2], [4], [5], [1], [2], [3], [2], [5], [3], [3], [5], [3], [2], [5], [3], [3], [5], [3], [1], [2], [3], [3], [2], [5], [1], [2], [2], [1], [4], [1], [4], [4], [1], [2], [1], [3], [5], [3], [2], [3], [2], [4], [3], [5], [3], [4], [2], [1], [2], [1], [4], [5], [2], [3], [3], [5], [1], [5], [3], [1], [5], [1], [1], [5], [1], [3], [3], [5], [4], [1], [3], [2], [5], [4], [1], [4], [1], [5], [3], [1], [5], [4], [2], [4], [2], [2], [4], [2], [1], [2], [1], [2], [1], [5], [2], [2], [5], [1], [1], [3], [4], [3], [3], [3], [4], [1], [4], [3], [2], [4], [5], [4], [1], [1], [2], [2], [3], [2], [4], [1], [5], [1], [3], [4], [5], [2], [1], [5], [1], [4], [3], [4], [2], [2], [3], [3], [1], [2], [4], [5], [3], [4], [2], [5], [1], [5], [1], [5], [3], [2], [1], [2], [1], [1], [5], [1], [3], [3], [2], [5], [4], [2], [1], [2], [5], [2], [2], [2], [3], [2], [3], [5], [5], [2], [1], [2], [3], [2], [4], [5], [2], [1], [1], [5], [2], [2], [3], [4], [5], [4], [3], [2], [1], [3], [2], [5], [4], [5], [4], [3], [1], [5], [2], [3], [2], [2], [3], [1], [4], [2], [2], [5], [5], [4], [1], [2], [5], [4], [4], [5], [5], [5], [3], [1], [3], [4], [2], [5], [3], [2], [5], [3], [3], [1], [1], [2], [3], [5], [2], [1], [2], [2], [1], [2], [3], [3], [3], [1], [4], [4], [2], [4], [1], [5], [2], [3], [2], [5], [2], [3], [5], [3], [2], [4], [2], [1], [1], [2], [1], [1], [5], [1], [1], [1], [4], [2], [2], [2], [3], [1], [1], [2], [4], [2], [3], [1], [3], [4], [2], [1], [5], [2], [3], [4], [2], [1], [2], [3], [2], [2], [1], [5], [4], [3], [4], [2], [1], [2], [5], [4], [4], [2], [1], [1], [5], [3], [3], [3], [1], [3], [4], [4], [5], [3], [4], [5], [2], [1], [1], [4], [2], [1], [1], [3], [1], [1], [2], [1], [5], [4], [3], [1], [3], [4], [2], [2], [2], [4], [2], [2], [1], [1], [1], [1], [2], [4], [5], [1], [1], [4], [2], [4], [5], [3], [1], [2], [3], [2], [4], [4], [3], [4], [2], [1], [2], [5], [1], [3], [5], [1], [1], [3], [4], [5], [4], [1], [3], [2], [5], [3], [2], [5], [1], [1], [4], [3], [5], [3], [5], [3], [4], [3], [5], [1], [2], [1], [5], [1], [5], [4], [2], [1], [3], [5], [3], [5], [5], [5], [3], [5], [4], [3], [4], [4], [1], [1], [4], [4], [1], [5], [5], [1], [4], [5], [1], [1], [4], [2], [3], [4], [2], [1], [5], [1], [5], [3], [4], [5], [5], [2], [5], [5], [1], [4], [4], [3], [1], [4], [1], [3], [3], [5], [4], [2], [4], [4], [4], [2], [3], [3], [1], [4], [2], [2], [5], [5], [1], [4], [2], [4], [5], [1], [4], [3], [4], [3], [2], [3], [3], [2], [1], [4], [1], [4], [3], [5], [4], [1], [5], [4], [1], [3], [5], [1], [4], [1], [1], [3], [5], [2], [3], [5], [2], [2], [4], [2], [5], [4], [1], [4], [3], [4], [3], [2], [3], [5], [1], [2], [2], [2], [5], [1], [2], [5], [5], [1], [5], [3], [3], [3], [1], [1], [1], [4], [3], [1], [3], [3], [4], [3], [1], [2], [5], [1], [2], [2], [4], [2], [5], [5], [5], [2], [5], [5], [3], [4], [2], [1], [4], [1], [1], [3], [2], [1], [4], [2], [1], [4], [1], [1], [5], [1], [2], [1], [2], [4], [3], [4], [2], [1], [1], [2], [2], [2], [2], [3], [1], [2], [4], [2], [1], [3], [2], [4], [2], [1], [2], [3], [5], [1], [2], [3], [2], [5], [2], [2], [2], [1], [3], [5], [1], [3], [1], [3], [3], [2], [2], [1], [4], [5], [1], [5], [2], [2], [2], [4], [1], [4], [3], [4], [4], [4], [1], [4], [4], [5], [5], [4], [1], [5], [4], [1], [1], [2], [5], [4], [2], [1], [2], [3], [2], [5], [4], [2], [3], [2], [4], [1], [2], [5], [2], [3], [1], [5], [3], [1], [2], [1], [3], [3], [1], [5], [5], [2], [2], [1], [4], [4], [1], [5], [4], [4], [2], [1], [5], [4], [1], [1], [2], [5], [2], [2], [2], [5], [1], [5], [4], [4], [4], [3], [4], [4], [5], [5], [1], [1], [3], [2], [5], [1], [3], [5], [4], [3], [4], [4], [2], [5], [3], [4], [3], [3], [1], [3], [3], [5], [4], [1], [3], [1], [5], [3], [2], [2], [3], [1], [1], [1], [5], [4], [4], [2], [5], [1], [3], [4], [3], [5], [4], [4], [2], [2], [1], [2], [2], [4], [3], [5], [2], [2], [2], [2], [2], [4], [1], [3], [4], [4], [2], [2], [5], [3], [5], [1], [4], [1], [5], [1], [4], [1], [2], [1], [3], [3], [5], [2], [1], [3], [3], [1], [5], [3], [2], [4], [1], [2], [2], [2], [5], [5], [4], [4], [2], [2], [5], [1], [2], [5], [4], [4], [2], [2], [1], [1], [1], [3], [3], [1], [3], [1], [2], [5], [1], [4], [5], [1], [1], [2], [2], [4], [4], [1], [5], [1], [5], [1], [5], [3], [5], [5], [4], [5], [2], [2], [3], [1], [3], [4], [2], [3], [1], [3], [1], [5], [1], [3], [1], [1], [4], [5], [1], [3], [1], [1], [2], [4], [5], [3], [4], [5], [3], [5], [3], [5], [5], [4], [5], [3], [5], [5], [4], [4], [1], [1], [5], [5], [4], [5], [3], [4], [5], [2], [4], [1], [2], [5], [5], [4], [5], [4], [2], [5], [1], [5], [2], [1], [2], [1], [3], [4], [5], [3], [2], [5], [5], [3], [2], [5], [1], [3], [1], [2], [2], [2], [2], [2], [5], [4], [1], [5], [5], [2], [1], [4], [4], [5], [1], [2], [3], [2], [3], [2], [2], [5], [3], [2], [2], [4], [3], [1], [4], [5], [3], [2], [2], [1], [5], [3], [4], [2], [2], [3], [2], [1], [5], [1], [5], [4], [3], [2], [2], [4], [2], [2], [1], [2], [4], [5], [3], [2], [3], [2], [1], [4], [2], [3], [5], [4], [2], [5], [1], [3], [3], [1], [3], [2], [4], [5], [1], [1], [4], [2], [1], [5], [4], [1], [3], [1], [2], [2], [2], [3], [5], [1], [3], [4], [2], [2], [4], [5], [5], [4], [4], [1], [1], [5], [4], [5], [1], [3], [4], [2], [1], [5], [2], [2], [5], [1], [2], [1], [4], [3], [3], [4], [5], [3], [5], [2], [2], [3], [1], [4], [1], [1], [1], [3], [2], [1], [2], [4], [1], [2], [2], [1], [3], [4], [1], [2], [4], [1], [1], [2], [2], [2], [2], [3], [5], [4], [2], [2], [1], [2], [5], [2], [5], [1], [3], [2], [2], [4], [5], [2], [2], [2], [3], [2], [3], [4], [5], [3], [5], [1], [4], [3], [2], [4], [1], [2], [2], [5], [4], [2], [2], [1], [1], [5], [1], [3], [1], [2], [1], [2], [3], [3], [2], [3], [4], [5], [1], [2], [5], [1], [3], [3], [4], [5], [2], [3], [3], [1], [4], [2], [1], [5], [1], [5], [1], [2], [1], [3], [5], [4], [2], [1], [3], [4], [1], [5], [2], [1], [5], [1], [4], [1], [4], [3], [1], [2], [5], [4], [4], [3], [4], [5], [4], [1], [2], [4], [2], [5], [1], [4], [3], [3], [3], [3], [5], [5], [5], [2], [3], [3], [1], [1], [4], [1], [3], [2], [2], [4], [1], [4], [2], [4], [3], [3], [1], [2], [3], [1], [2], [4], [2], [2], [5], [5], [1], [2], [4], [4], [3], [2], [3], [1], [5], [5], [3], [3], [2], [2], [4], [4], [1], [1], [3], [4], [1], [4], [2], [1], [2], [3], [1], [5], [2], [4], [3], [5], [4], [2], [1], [5], [4], [4], [5], [3], [4], [5], [1], [5], [1], [1], [1], [3], [4], [1], [2], [1], [1], [2], [4], [1], [2], [5], [3], [4], [1], [3], [4], [5], [3], [1], [3], [4], [2], [5], [1], [3], [2], [4], [4], [4], [3], [2], [1], [3], [5], [4], [5], [1], [4], [2], [3], [5], [4], [3], [1], [1], [2], [5], [2], [2], [3], [2], [2], [3], [4], [5], [3], [5], [5], [2], [3], [1], [3], [5], [1], [5], [3], [5], [5], [5], [2], [1], [3], [1], [5], [4], [4], [2], [3], [5], [2], [1], [2], [3], [3], [2], [1], [4], [4], [4], [2], [3], [3], [2], [1], [1], [5], [2], [1], [1], [3], [3], [3], [5], [3], [2], [4], [2], [3], [5], [5], [2], [1], [3], [5], [1], [5], [3], [3], [2], [3], [1], [5], [5], [4], [4], [4], [4], [3], [4], [2], [4], [1], [1], [5], [2], [4], [5], [2], [4], [1], [4], [5], [5], [3], [3], [1], [2], [2], [4], [5], [1], [3], [2], [4], [5], [3], [1], [5], [3], [3], [4], [1], [3], [2], [3], [5], [4], [1], [3], [5], [5], [2], [1], [4], [4], [1], [5], [4], [3], [4], [1], [3], [3], [1], [5], [1], [3], [1], [4], [5], [1], [5], [2], [2], [5], [5], [5], [4], [1], [2], [2], [3], [3], [2], [3], [5], [1], [1], [4], [3], [1], [2], [1], [2], [4], [1], [1], [2], [5], [1], [1], [4], [1], [2], [3], [2], [5], [4], [5], [3], [2], [5], [3], [5], [3], [3], [2], [1], [1], [1], [4], [4], [1], [3], [5], [4], [1], [5], [2], [5], [3], [2], [1], [4], [2], [1], [3], [2], [5], [5], [5], [3], [5], [3], [5], [1], [5], [1], [3], [3], [2], [3], [4], [1], [4], [1], [2], [3], [4], [5], [5], [3], [5], [3], [1], [1], [3], [2], [4], [1], [3], [3], [5], [1], [3], [3], [2], [4], [4], [2], [4], [1], [1], [2], [3], [2], [4], [1], [4], [3], [5], [1], [2], [1], [5], [4], [4], [1], [3], [1], [2], [1], [2], [1], [1], [5], [5], [2], [4], [4], [2], [4], [2], [2], [1], [1], [3], [1], [4], [1], [4], [1], [1], [2], [2], [4], [1], [2], [4], [4], [3], [1], [2], [5], [5], [4], [3], [1], [1], [4], [2], [4], [5], [5], [3], [3], [2], [5], [1], [5], [5], [2], [1], [3], [4], [2], [1], [5], [4], [3], [3], [1], [1], [2], [2], [2], [2], [2], [5], [2], [3], [3], [4], [4], [5], [3], [5], [2], [3], [1], [1], [2], [4], [2], [4], [1], [2], [2], [3], [1], [1], [3], [3], [5], [5], [3], [2], [3], [3], [2], [4], [3], [3], [3], [3], [3], [5], [5], [4], [3], [1], [3], [1], [4], [1], [1], [1], [5], [4], [5], [4], [1], [4], [1], [1], [5], [5], [2], [5], [5], [3], [2], [1], [4], [4], [3], [2], [1], [2], [5], [1], [3], [5], [1], [1], [2], [3], [4], [4], [2], [2], [1], [3], [5], [1], [1], [3], [5], [4], [1], [5], [2], [3], [1], [3], [4], [5], [1], [3], [2], [5], [3], [5], [3], [1], [3], [2], [2], [3], [2], [4], [1], [2], [5], [2], [1], [1], [5], [4], [3], [4], [3], [3], [1], [1], [1], [2], [4], [5], [2], [1], [2], [1], [2], [4], [2], [2], [2], [2], [1], [1], [1], [2], [2], [5], [2], [2], [2], [1], [1], [1], [4], [2], [1], [1], [1], [2], [5], [4], [4], [4], [3], [2], [2], [4], [2], [4], [1], [1], [3], [3], [3], [1], [1], [3], [3], [4], [2], [1], [1], [1], [1], [2], [1], [2], [2], [2], [2], [1], [3], [1], [4], [4], [1], [4], [2], [5], [2], [1], [2], [4], [4], [3], [5], [2], [5], [2], [4], [3], [5], [3], [5], [5], [4], [2], [4], [4], [2], [3], [1], [5], [2], [3], [5], [2], [4], [1], [4], [3], [1], [3], [2], [3], [3], [2], [2], [2], [4], [3], [2], [3], [2], [5], [3], [1], [3], [3], [1], [5], [4], [4], [2], [4], [1], [2], [2], [3], [1], [4], [4], [4], [1], [5], [1], [3], [2], [3], [3], [5], [4], [2], [4], [1], [5], [5], [1], [2], [5], [4], [4], [1], [5], [2], [3], [3], [3], [4], [4], [2], [3], [2], [3], [3], [5], [1], [4], [2], [4], [5], [4], [4], [1], [3], [1], [1], [3], [5], [5], [2], [3], [3], [1], [2], [2], [4], [2], [4], [4], [1], [2], [3], [1], [2], [2], [1], [4], [1], [4], [5], [1], [1], [5], [2], [4], [1], [1], [3], [4], [2], [3], [1], [1], [3], [5], [4], [4], [4], [2], [1], [5], [5], [4], [2], [3], [4], [1], [1], [4], [4], [3], [2], [1], [5], [5], [1], [5], [4], [4], [2], [2], [2], [1], [1], [4], [1], [2], [4], [2], [2], [1], [2], [3], [2], [2], [4], [2], [4], [3], [4], [5], [3], [4], [5], [1], [3], [5], [2], [4], [2], [4], [5], [4], [1], [2], [2], [3], [5], [3], [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2EqkdBCWvvL",
        "outputId": "3d8b0db6-5a53-4710-d244-d6a4c3b836f8"
      },
      "source": [
        "# 데이터 4분할\r\n",
        "C_tokenized = np.array(C_tokenized) # numpy로 변환\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(A_tokenized, #  입력 data\r\n",
        "                                                    C_tokenized, # 출력 data\r\n",
        "                                                    train_size=MY_SPLIT,\r\n",
        "                                                    shuffle=False)\r\n",
        "\r\n",
        "# 데이터 모양 확인 \r\n",
        "print('학습용 입력 데이터 모양 :', X_train.shape)\r\n",
        "print('학습용 출력 데이터 모양 :', Y_train.shape)\r\n",
        "\r\n",
        "print('평가용 입력 데이터 모양 :', X_test.shape)\r\n",
        "print('평가용 출력 데이터 모양 :', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습용 입력 데이터 모양 : (1780, 200)\n",
            "학습용 출력 데이터 모양 : (1780, 1)\n",
            "평가용 입력 데이터 모양 : (445, 200)\n",
            "평가용 출력 데이터 모양 : (445, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9tAGX52k5uH"
      },
      "source": [
        "3. 인공 신경망 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag_-Dgfnk_c5",
        "outputId": "b7dc856f-52e0-42f0-bc23-a36cb450c90d"
      },
      "source": [
        "# RNN 구현\r\n",
        "model = Sequential()\r\n",
        "\r\n",
        "model.add(Embedding(input_dim=MY_VOCAB, # MY_VOCAB개를\r\n",
        "                    output_dim=MY_EMBED)) # MY_EMBED개로\r\n",
        "\r\n",
        "model.add(Dropout(rate=0.5)) # Dropout : 임의의 뉴런의 출력을 일부로 0으로 만드는 작업\r\n",
        "                             # 과적합(overfitting) 방지하기 위해 뉴런 몇 개를 죽임\r\n",
        "\r\n",
        "model.add(Bidirectional(LSTM(units=MY_HIDDEN)))\r\n",
        "\r\n",
        "model.add(Dense(units=6,\r\n",
        "                activation='softmax'))\r\n",
        "\r\n",
        "print('RNN 요약')\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN 요약\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 64)          320000    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 64)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 200)               132000    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 6)                 1206      \n",
            "=================================================================\n",
            "Total params: 453,206\n",
            "Trainable params: 453,206\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHEtRrkZqwE3"
      },
      "source": [
        "4. 인공 신경망 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cnRdeBLqzO3",
        "outputId": "fbe232d4-009d-4c2a-fb5d-d10be1554c0c"
      },
      "source": [
        "# RNN 학습\r\n",
        "model.compile(optimizer='adam',\r\n",
        "              loss='sparse_categorical_crossentropy',\r\n",
        "              metrics=['acc'])\r\n",
        "\r\n",
        "print(' 학습 시작')\r\n",
        "begin = time()\r\n",
        "\r\n",
        "model.fit(x=X_train,\r\n",
        "          y=Y_train,\r\n",
        "          epochs=MY_EPOCH,\r\n",
        "          verbose=1)\r\n",
        "\r\n",
        "end = time()\r\n",
        "print('학습 시간 : {:.2f}'.format(end - begin))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 학습 시작\n",
            "Epoch 1/100\n",
            "56/56 [==============================] - 11s 25ms/step - loss: 1.6683 - acc: 0.2414\n",
            "Epoch 2/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.3684 - acc: 0.4860\n",
            "Epoch 3/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.6220 - acc: 0.7836\n",
            "Epoch 4/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2995 - acc: 0.9235\n",
            "Epoch 5/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.2123 - acc: 0.9343\n",
            "Epoch 6/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.2476 - acc: 0.9288\n",
            "Epoch 7/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0538 - acc: 0.9900\n",
            "Epoch 8/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0727 - acc: 0.9880\n",
            "Epoch 9/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0129 - acc: 0.9992\n",
            "Epoch 10/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0106 - acc: 0.9991\n",
            "Epoch 11/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0110 - acc: 0.9980\n",
            "Epoch 12/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0480 - acc: 0.9866\n",
            "Epoch 15/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0021 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0012 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.1522e-04 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 8.8415e-04 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 6.8097e-04 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 6.2594e-04 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.6455e-04 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 4.9761e-04 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 5.8947e-04 - acc: 0.9999\n",
            "Epoch 29/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.0132 - acc: 0.9975\n",
            "Epoch 30/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 0.0016 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "56/56 [==============================] - 1s 24ms/step - loss: 8.5254e-04 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.3985e-04 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0100 - acc: 0.9978\n",
            "Epoch 34/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.9259e-04 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.9651e-04 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.8336e-04 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.2333e-04 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.9226e-04 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.9825e-04 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.5385e-04 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.1277e-04 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.3671e-04 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.2732e-04 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.4404e-04 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0221 - acc: 0.9949\n",
            "Epoch 46/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.5015e-04 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.3158e-04 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.5312e-04 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.2002e-04 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.8430e-04 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.8601e-04 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.6045e-04 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.4952e-04 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.3932e-04 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.2628e-04 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.1775e-04 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.0612e-04 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.7815e-05 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.0024e-04 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.4113e-05 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 8.3314e-05 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.7504e-04 - acc: 0.9999\n",
            "Epoch 63/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0115 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.5221e-04 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.5276e-04 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.0680e-04 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.9892e-04 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.3551e-04 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.1823e-04 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.0295e-04 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 8.7159e-05 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 8.2468e-05 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 7.9538e-05 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.7911e-05 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.8682e-05 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "56/56 [==============================] - 1s 22ms/step - loss: 5.6478e-05 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.0089e-05 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.4860e-05 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.3849e-05 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 4.2813e-05 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.8330e-05 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.9943e-05 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 7.2967e-04 - acc: 0.9999\n",
            "Epoch 84/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0450 - acc: 0.9870\n",
            "Epoch 85/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0358 - acc: 0.9860\n",
            "Epoch 86/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 0.0014 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.9727e-04 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 3.0516e-04 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 2.2505e-04 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.6892e-04 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.5398e-04 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.2101e-04 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 1.0669e-04 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 9.9634e-05 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 8.4173e-05 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 7.5292e-05 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 7.7139e-05 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.1508e-05 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 6.1826e-05 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "56/56 [==============================] - 1s 23ms/step - loss: 5.1410e-05 - acc: 1.0000\n",
            "학습 시간 : 138.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S83BcC2Ys03B"
      },
      "source": [
        "5. 인공 신경망 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KBwEDtqsziO",
        "outputId": "27e7c260-b2d1-41aa-aff9-56c58c4b74c5"
      },
      "source": [
        "# RNN 평가\r\n",
        "score = model.evaluate(X_test,\r\n",
        "                       Y_test,\r\n",
        "                       verbose=0)\r\n",
        "\r\n",
        "print('최종 손실값 : {:.2f}'.format(score[0]))\r\n",
        "print('최종 정확도 : {:.2f}'.format(score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 손실값 : 0.22\n",
            "최종 정확도 : 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}